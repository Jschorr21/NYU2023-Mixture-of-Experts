#!/usr/bin/env python
# coding: utf-8
""" LLM Evaluator on MMLU Benchmark

This script loads in multiple pre-trained Hugging Face Language models from the inputted paths to create a mixture model. 
The mixture model is evaluated on the MMLU benchmark which has multiple choice questions from 57 different subjects/topics
ranging from high school mathematics to college genetics to professional law. For each question in the test set, a prompt 
string is generated containing 5 example questions and answers from the given subject's dev split along with the question 
for which the model is to determine the answer. For each question, the mixture model averages the probability distributions 
for the next token generated by each model to produce a composite prediction. The script will evaluate the accuracy of the mixture
model by comparing the generated answers to the actual answers from MMLU. The generated answers, correct answers, and accuracy 
metrics by subset (and overall) will be saved to the inputted json file path.


Usage: 
    python Model_Eval.py --HFmodel_paths path/to/HF_model1 path/to/HF_model2 path/to/HF_model3 --output_path path/to/output_file

Parameters:
    --HFmodel_paths (list): The paths to the directories containing the pre-trained Hugging Face models.
    --output_path (str): The path to the output JSON file where the results will be saved.

Author: 
    Jake Schorr
"""

import torch
import re
import json
import transformers
from datasets import load_dataset
from transformers import LlamaTokenizer, LlamaForCausalLM
import argparse
from torch.nn.utils.rnn import pad_sequence
from utils import subsets, map_to_ABCD, get_dev_and_test_data, gen_examples, gen_prompt, clean_pred, evaluate, write_results


def load_model_and_tokenizer(model_path, device_idx):
    """
    Loads the model and tokenizer from Hugging Face corresponding to parameter 'model_path'
    Moves given model to cuda device represented by device_idx

    Parameters
    ----------
    model_path : str
        Hugging Face model path for desired model
    device_idx : int
        index of cuda device to load model onto
    
    Returns
    ----------
    model
        loaded model from 'model_path'
    tokenizer
        loaded tokenizer from 'model_path'
    """
    tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path)
    model = transformers.LlamaForCausalLM.from_pretrained(model_path)
    model.to(f'cuda:{device_idx}')
    return model, tokenizer


def get_pred(new_token_probs, tokenizer):
    """
    Stacks the probability distributions from each model, takes the mean, and 
    computes the most likely new token using torch.argmax. This next token is 
    decoded and cleaned and returned

    Parameters
    ----------
    new_token_probs : list
        list containing probability distribution tensors from each model
    tokenizer : HF tokenizer
        tokenizer to decode the new token
    
    Returns
    ----------
    pred_s
        cleaned composite prediction
    """
    new_token_probs_t = pad_sequence(new_token_probs,batch_first = True)
    new_token_mean_probs = torch.mean(new_token_probs_t, dim = 0)
    new_tokens = torch.argmax(new_token_mean_probs, dim=-1)
    pred = tokenizer.decode(new_tokens, skip_special_tokens=True)
    pred_s = clean_pred(pred)
    return pred_s

def run_mixture_model(model_paths, data_path = 'cais/mmlu'):
    """
    This function performs the inference on the mixture model and evaluation dataset. 
    It calls the function to load in the model and tokenizer, and loops through the
    subsets of the data set prompting each model to answer each question for each subset,
    and computing a composite answer based on each model's probability distribution 
    for the new token. The selected answers are recorded.

    Parameters
    ----------
    model_path : str
        Hugging Face model path for desired model
    data_path : str
        Hugging Face path for desired evaluation dataset. Defaults to 'cais/mmlu'
    
    Returns
    ----------
    preds_dict
        dictionary containing answers provided by the mixture model
    gold_dict
        dictionary containing the correct answers from Hugging Face
    """
    models_and_tokenizers = []
    count = 0
    for model_path in model_paths:
        model, tokenizer = load_model_and_tokenizer(model_path, count)
        models_and_tokenizers.append((model, tokenizer))
        count = count+1
    preds_dict = {}
    gold_dict = {}
    for subset in subsets:
        print(f"Beginning subset: {subset} ----------------\n\n")
        test_data, dev_data = get_dev_and_test_data(data_path, subset)
        examples = gen_examples(dev_data)
        preds = []
        for i in range(len(test_data)):
            prompt = ''
            prompt = gen_prompt(test_data, examples, i)
            new_token_probs = []
            for model, tokenizer in models_and_tokenizers:
                input_ids = tokenizer(prompt, return_tensors='pt').to(model.device)
                output = model.generate(input_ids['input_ids'], max_new_tokens=1, return_dict_in_generate=True, output_scores=True)
                probs = torch.softmax(output['scores'][0], dim = -1)
                new_token_probs.append(probs[0])
            pred_s = get_pred(new_token_probs, tokenizer)
            preds.append(pred_s)
        preds_dict[subset] = preds
        gold_dict[subset] = test_data['answer']
        torch.cuda.empty_cache()
    return preds_dict, gold_dict


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--hf_model_paths', help = "path(s) to HF model(s)", nargs='+', type=str, required=True)
    parser.add_argument('--output_path', type=str, help = "path to output file", required=True)
    args = parser.parse_args()
    preds_dict, gold_dict = mixture_model(args.hf_model_paths, 'cais/mmlu')
    write_results(args.output_path, preds_dict, gold_dict)


